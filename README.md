# Chat with PDF using Offline
Welcome to the Chat with PDF repository! This project allows you to interact with PDF documents using a locally deployed Llama 2.0 Large Language Model (LLM). The model runs offline on a 4GB GPU, ensuring privacy and security for your data.

Features

Offline PDF Interaction: Query and interact with PDF files without an internet connection.

Llama 2.0 Integration: Utilizes the state-of-the-art Llama 2.0 language model for natural language understanding.

GPU-Optimized: Runs on a 4GB GPU for efficient performance.

Privacy-Focused: All processing occurs locally, ensuring data privacy.

Requirements

Python 3.10+

CUDA-enabled GPU with 4GB VRAM

PyTorch

Llama 2.0 weights (downloaded separately)

Download offline model:
https://huggingface.co/Manel/Llama-2-13b-chat-hf-Q2_K-GGUF/tree/main

How to run this repo:
1. Install all required packages from the environment.yaml file
2. Run File_Extract.py as the main file.
3. Keep folders and file names as listed in the FileExtract.py file 

